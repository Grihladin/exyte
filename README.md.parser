# PDF Compliance Document Parser

Parse the **2021 International Building Code** into structured JSON with chapter/section hierarchy, section text, numbered lists, references, and (partial) table captures.

## Current Status

| Phase | Status | Notes |
| --- | --- | --- |
| Phase 1 â€“ Core PDF extraction | âœ… Complete | PyMuPDF wrapper, metadata models, CLI scaffolding |
| Phase 2 â€“ Structure parsing & references | âœ… Complete | Chapter/section detection, numbered items, reference tagging, section metadata |
| Phase 3 â€“ Table extraction | ðŸš§ Partial | Camelot integration with heuristics; validator highlights missing tables |
| Phase 4 â€“ Figures & advanced metadata | ðŸš§ Planned |
| Phase 5 â€“ QA & performance tuning | ðŸš§ Planned |

## Quick Start

```bash
# Install deps (once)
uv sync

# Parse default PDF (pages 32â€‘769) with progress bar + events log
cd parser
UV_CACHE_DIR=.uv-cache uv run python -m src.main

# Parse custom PDF, first 100 pages, starting at page 50 (1-indexed)
UV_CACHE_DIR=.uv-cache uv run python -m src.main /path/to/file.pdf 100 --start=50

# Phase 1 only (text extraction preview)
UV_CACHE_DIR=.uv-cache uv run python -m src.main --phase1 5 --start=10
```

### Local database (Docker)

The project includes a `docker-compose.yml` for running PostgreSQL with `pgvector` locally. By default the container maps the DB port to `55432` to avoid collisions with any local PostgreSQL instances.

Start the local DB:

```bash
docker compose up -d
```

If you change the port in `.env`, recreate the container with `docker compose down -v` followed by `docker compose up -d` to re-run init scripts and seed the DB.

If you already have a running local Postgres instance on port 5432, either stop it before using the Docker Compose stack or update `.env` to set `POSTGRES_PORT` to an unused port (the `.env.example` shows `55432`).

### Outputs

| Artifact | Description |
| --- | --- |
| `parser/output/parsed_document.json` | Structured document (chapters â†’ sections â†’ references) |
| `parser/output/events.log` | Line-by-line event history (detected chapters, table hints, etc.) |
| `validate_output.py` | JSON sanity check for missing table data (`uv run python validate_output.py`) |

### CLI Flags

- `num_pages` (positional) â€“ limit how many pages to parse (defaults to the entire configured slice)
- `--start=<page>` â€“ 1-indexed start page
- `--phase1` â€“ run only the Phaseâ€‘1 PDF sampler (text/blocks/images)

Progress is rendered with `tqdm`; no additional logging is emitted to stdout.

## Key Components

| Module | Purpose |
| --- | --- |
| `src/main.py` | Minimal CLI argument parser that delegates to the pipeline |
| `src/pipeline.py` | Full structure parsing loop, Camelot integration, progress bar + event logging |
| `src/pipeline_pdf.py` | Lightweight Phaseâ€‘1 sampler reusing the same progress UX |
| `src/parsers/pdf_extractor.py` | PyMuPDF wrapper with header/footer filtering (`pdf_filters.py`) |
| `src/parsers/structure_parser.py` | Chapter/section detection, numbered items, depth tracking |
| `src/parsers/structure_utils.py` | Helper utilities for structure parsing heuristics |
| `src/parsers/reference_extractor.py` | Extract internal sections, tables, figures, external docs |
| `src/parsers/table_extractor.py` | Camelot wrapper (tries lattice â†’ stream, filters low-quality tables) |
| `src/parsers/metadata_collector.py` | Section-level metadata aggregation |
| `src/models/*` | Pydantic models for documents, sections, references, metadata |

## Table Coverage & Validation

Table detection can be noisyâ€”Camelot only runs when a `TABLE ...` label exists on the page. Use:

```bash
UV_CACHE_DIR=.uv-cache uv run python validate_output.py
```

The script reports table references whose `table_data` is still missing, so you can triage problematic pages/table layouts.

## Implementation Notes

- **Progress UX**: All parsing happens inside `tqdm` contexts; stdout stays clean while long runs monitorable.
- **Event logging**: `parser/output/events.log` captures chapter detections, table hints, and attachments for deeper debugging without spamming the console.
- **Flat section model**: Sections store their numbering depth but no longer nest subsections; relationships are encoded via `Section.depth` and sequential order.
- **Camelot heuristics**: We gate table extraction with textual hints, filter out 1-column/empty grids, and assign tables either to sections on the same page or (when a page is pure table) to the last seen section.
- **Header/footer filtering**: `HeaderFooterFilter` automatically removes repeated headers, footers, and copyright blocks when `PDFExtractor(remove_headers_footers=True)` is used.

## Testing

```bash
# Smoke test for part parsing regression
UV_CACHE_DIR=.uv-cache uv run python test_part_fix.py

# Validate final JSON for table coverage
UV_CACHE_DIR=.uv-cache uv run python validate_output.py
```

## Dependencies

- `PyMuPDF (fitz)` â€“ PDF text / block / image extraction
- `pydantic` â€“ strongly typed models
- Custom table snapshotting built on PyMuPDF with pdfplumber-based table parsing (tables saved as images for reference and converted directly to Markdown)
- `tqdm` â€“ progress bars for both phases

Dependency versions are pinned in `pyproject.toml`; install via `uv sync`.

## Parser Utility Scripts

- `python -m src.scripts.run_parse_media [PDF] [--pages N] [--start M]` â€“ run structure parsing, extract figures, and save table snapshots plus Markdown.
- `python -m src.scripts.run_table_ocr [--source parser/output/parsed_document.json] [--tables TABLE_ID ...]` â€“ rebuild Markdown for stored tables using pdfplumber and saved bounding boxes.
- `python -m src.scripts.run_embeddings [--source parser/output/parsed_document.json]` â€“ execute the ingestion pipeline with embeddings enabled (writes to the configured database).

Embedding notes:

- To generate embeddings (used by the RAG ingestion) you will need OpenAI credentials set in `.env` as `OPENAI_API_KEY`.
- If `langchain-openai` isn't installed the project will try to use the `openai` SDK directlyâ€”install both via `uv sync` (or `pip install langchain-openai openai`).
- To skip embeddings during ingestion (for offline/dev/testing), run:

```bash
UV_CACHE_DIR=.uv-cache uv run -m rag.scripts.ingest_document parser/output/parsed_document.json --skip-embeddings
```

Or to allow deterministic fallback embeddings (deterministic pseudo-embeddings used for dev), run with:

```bash
UV_CACHE_DIR=.uv-cache uv run -m rag.scripts.ingest_document parser/output/parsed_document.json --allow-embed-fallback
```
